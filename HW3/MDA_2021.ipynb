{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# MDA 2021\n",
        "## Pyspark Sample Code\n",
        "-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "## Setup\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qHai2252mI",
        "outputId": "b0411162-bc70-43b0-e708-f42c94a808a0"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to processing data\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K93ABEy9Zlo",
        "outputId": "8edeb2a1-35ba-463a-f36d-bcca540bce87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bK7ob3k4Yd1"
      },
      "source": [
        "## Check and extract data\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT70OpRGzkWh",
        "outputId": "cee8742c-a408-4630-da62-fee1eaa0a987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " checkstatus.csv   MDA_2021.ipynb       SystemID.csv\n",
            " CompanyID.csv\t   Sample_Data.zip      Traffic.csv\n",
            " Data.zip\t   Sample_Traffic.csv  'پروژه MDA2021.pdf'\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive/Colab Notebooks/MDA/HW3/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLoUDlZrzoAg",
        "outputId": "8d6c6cca-23ff-42bc-a6c4-99e93dcc4720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ./Sample_Data.zip\n",
            "  inflating: ./Sample_Traffic.csv    \n"
          ]
        }
      ],
      "source": [
        "# !unzip \"/content/drive/My Drive/Colab Notebooks/MDA/HW3/Sample_Data.zip\" -d \"/content/drive/My Drive/Colab Notebooks/MDA/HW3\"\n",
        "!unzip \"./Sample_Data.zip\" -d \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "the cells above, extract data which is in '/content/drive/My Drive/Test' to /content/drive/My Drive/Test/Traffic.csv  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "## Initializing Spark and read data\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lDh957r_0snm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21/12/05 20:48:27 WARN Utils: Your hostname, amin-X556UQK resolves to a loopback address: 127.0.1.1; using 192.168.1.15 instead (on interface wlp3s0)\n",
            "21/12/05 20:48:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/12/05 20:48:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType\n",
        "from pyspark.sql.functions import col,current_timestamp,to_date,hour,dayofweek\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark_Processor\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc=spark.sparkContext\n",
        "\n",
        "schema = StructType([ \\\n",
        "        StructField(\"DEVICE_CODE\", IntegerType(), True), \n",
        "        StructField(\"SYSTEM_ID\",IntegerType(),True), \\\n",
        "        StructField(\"ORIGINE_CAR_KEY\",StringType(),True), \\\n",
        "        StructField(\"FINAL_CAR_KEY\",StringType(),True), \\\n",
        "        StructField(\"CHECK_STATUS_KEY\", IntegerType(), True), \\\n",
        "        StructField(\"COMPANY_ID\", StringType(), True), \\\n",
        "        StructField(\"PASS_DAY_TIME\", TimestampType(), True)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " در این قسمت، لطفا آدرس فایل \n",
        "csv\n",
        " را به جای آدرس قرار داده شده جایگذاری کنید.  \n",
        "  \n",
        "</h3> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGoJ3OTX3Lz_",
        "outputId": "4eb0eb19-071a-46d0-bbfe-9a00b87e0aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+---------------+-------------+----------------+----------+-------------------+\n",
            "|DEVICE_CODE|SYSTEM_ID|ORIGINE_CAR_KEY|FINAL_CAR_KEY|CHECK_STATUS_KEY|COMPANY_ID|      PASS_DAY_TIME|\n",
            "+-----------+---------+---------------+-------------+----------------+----------+-------------------+\n",
            "|     200501|       81|       10477885|     10477885|               5|       161|2021-06-01 03:54:39|\n",
            "|        155|       81|       87625017|     87625017|               5|       161|2021-06-01 04:14:21|\n",
            "|     631757|       81|        8652928|      8652928|               5|       161|2021-06-01 03:58:57|\n",
            "|     631757|       81|        8548123|      8548123|               5|       161|2021-06-01 04:01:38|\n",
            "|     631757|       81|       24715264|     24715264|               5|       161|2021-06-01 03:56:57|\n",
            "+-----------+---------+---------------+-------------+----------------+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# df=spark.read.csv('/content/drive/My Drive/Colab Notebooks/MDA/HW3/Sample_Traffic.csv',header=True,schema=schema)\n",
        "df=spark.read.csv('Sample_Traffic.csv',header=True,schema=schema)\n",
        "# df.select('DEVICE_CODE').distinct().count()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " با استفاده از یک بار فرایند \n",
        " map\n",
        " ، هر ردیف از \n",
        " df\n",
        " را به صورت کلید پلاک خودرو، و زمان عبور از مسیر در می‌آوریم. مقدار این کلید را نیز برابر کد دستگاه (\n",
        "     که نمایانگر مسیر عبوری است \n",
        " ) \n",
        " می‌گذاریم. \n",
        " rdd\n",
        " به دست آمده به ازای هر مسیر در هر روز، یک ردیف خواهد داشت. \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhYYY2TRjgCv",
        "outputId": "3240f0b9-8a19-4d4c-c33b-1386efe9f881"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "[(('10477885', datetime.date(2021, 6, 1)), 200501),\n",
              " (('87625017', datetime.date(2021, 6, 1)), 155),\n",
              " (('8652928', datetime.date(2021, 6, 1)), 631757),\n",
              " (('8548123', datetime.date(2021, 6, 1)), 631757),\n",
              " (('24715264', datetime.date(2021, 6, 1)), 631757)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd = df.rdd.map(lambda x: ((x['FINAL_CAR_KEY'], x['PASS_DAY_TIME'].date()),  x['DEVICE_CODE']))\n",
        "rdd.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " اگر \n",
        " rdd\n",
        " بالا را به کمک کلیدش \n",
        " group_by\n",
        " کنیم، برای هر ماشین در هر روز خاص، تمام مسیر‌هایی که عبور کرده در یک لیست خواهیم داشت. البته به دلیل ماهیت مسئله \n",
        " frequent_itemsets\n",
        " ، \n",
        " به جای لیست از\n",
        " set\n",
        " استفاده کرده‌ام، تا مسیر‌های تکراری نداشته باشیم. هرچند می‌توان مسئله را به گونه‌ای تعریف کرد که ترتیب و یا تعداد تکرار مسیرها تاثیر گذار باشد، اما با توجه به خروجی‌ای که ما می‌خوایم، این فرض چیزی از کلیت مسئله نمی‌کاهد.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTYvjMNku-d",
        "outputId": "9c392bc2-5afb-4fe3-975d-25c9b778df14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "[(('69939810', datetime.date(2021, 6, 1)), {206602, 631830, 900233}),\n",
              " (('11046172', datetime.date(2021, 6, 1)), {206602}),\n",
              " (('29077699', datetime.date(2021, 6, 1)),\n",
              "  {119,\n",
              "   128,\n",
              "   206602,\n",
              "   208602,\n",
              "   210602,\n",
              "   631367,\n",
              "   631763,\n",
              "   631829,\n",
              "   900135,\n",
              "   900233,\n",
              "   900234,\n",
              "   900240,\n",
              "   900246,\n",
              "   900266,\n",
              "   22010111,\n",
              "   22010123}),\n",
              " (('40682798', datetime.date(2021, 6, 1)), {206602}),\n",
              " (('48823778', datetime.date(2021, 6, 1)), {206602})]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_group = rdd.groupByKey().mapValues(set)\n",
        "rdd_group.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvfBcc4pouA3"
      },
      "source": [
        "# A-Priori"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " برای داشتن سبدها، نیازی به نگه داشتن پلاک و روز نداریم. بنابرین آن‌ها را نگه نمی‌داریم و سبدها را به دست می‌آوریم.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hcs4hiW_ovuP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "[{206602, 631830, 900233}, {206602}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "baskets_rdd = rdd_group.map(lambda x: x[1])\n",
        "baskets_rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " این قسمت از کد، برای تست کردن استفاده می‌شود، که به کمک آن می‌توانیم یک نمونه از داده مسئله بگیریم تا اجرای کد سریع تر باشد. متغیر\n",
        " THRESHOLD\n",
        " نیز نسبتی از کل سبدهاست که از آن بیشتر مجموعه را پر تکرار تشخیص می‌دهیم. در مورد نتایج و پارامترهای استفاده شده در انتهای کد توضیح خواهم داد. \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wjHJ97i0ck8e"
      },
      "outputs": [],
      "source": [
        "SAMPLE = True\n",
        "SAMPLE_SIZE = 0.01\n",
        "THRESHOLD = 0.001\n",
        "\n",
        "if SAMPLE:\n",
        "  baskets_rdd = baskets_rdd.sample(True, SAMPLE_SIZE)\n",
        "  baskets_rdd = baskets_rdd.coalesce(10)\n",
        "  baskets_rdd.cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " در دو بلاک بعدی، ابتدا تعداد کل سبدها را می‌شماریم، و سپس با استفاده از \n",
        " THRESHOLD\n",
        " ، حداقل ساپورت را به دست می‌آوریم. مقدار \n",
        " MIN_COUNT\n",
        " را نیز در انتهای گذارش ذکر خواهم کرد.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-r_XUYi2Q0c",
        "outputId": "42d0ae69-550e-4dd1-8ba0-7a8ed96feac7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "165"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BASKETS_COUNT = baskets_rdd.count()\n",
        "BASKETS_COUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzufIXYR1c4t",
        "outputId": "82807417-f89b-48fb-d168-278d1f36ed1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MIN_COUNT = int(THRESHOLD * BASKETS_COUNT)\n",
        "MIN_COUNT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " در این مرحله ابتدا تعداد تکرار هر یک از \n",
        " item\n",
        " ها را به دست می‌آورم و سپس با کمک حداقل ساپورت، \n",
        " item\n",
        " های پر تکرار را به دست می‌آورم.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpmSW_2j0h95",
        "outputId": "4fce733b-f748-4b55-a56d-2952085aa0a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[((631363,), 2),\n",
              " ((900171,), 3),\n",
              " ((119,), 5),\n",
              " ((22010087,), 1),\n",
              " ((22010095,), 1)]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frequent_items_rdd = baskets_rdd.flatMap(lambda basket: [((item,),1) for item in basket]).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] > MIN_COUNT)\n",
        "frequent_items_rdd.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " این کد تعداد \n",
        " item\n",
        " های پرتکرار را محاسبه و چاپ می کند. \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsb2aEew1uj2",
        "outputId": "cf9d6dec-b5b2-4a21-f72b-5f02d4c5a206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "202"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FREQUENT_ITEMS_COUNT = frequent_items_rdd.count()\n",
        "FREQUENT_ITEMS_COUNT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " <ul dir=\"rtl\">\n",
        "  <li dir=\"rtl\">\n",
        "  generate_combinations \n",
        "    \n",
        "  </li> \n",
        "  این تابع، به عنوان ورودی \n",
        "  itemset\n",
        "  های پرتکرار مرحله‌ی قبلی را می‌گیرد، و کاندید‌های مرحله‌ی بعد را می‌سازد. این کار به به این صورت انجام می‌دهد که ابتدا \n",
        "  item\n",
        "  های پرتکرار باقی‌مانده در کل \n",
        "  itemset\n",
        "  های پرتکرار را به دست می‌آورد و مرتب می‌کند. سپس به ازای هر \n",
        "  itemset\n",
        "  پرتکرار، بزرگترین عنصر آن را می‌گیرد، و هر عضو مجموعه‌ی \n",
        "  item\n",
        "  های باقی مانده که از این عضو بزرگتر است را به انتهای این مجموعه اضافه می‌کند و یک کاندید جدید در نظر می‌گیرد. منطق این تابع به این صورت است که نیازی نیست عناصر کوچک‌تر را به این مجموعه‌ها اضافه کنیم. برای مثال فرض کنید مجموعه‌ی \n",
        "  [b,c,d]\n",
        "  در مرحله‌ی قبلی پرتکرار بوده باشد، و مجموعه‌ی عناصر پرتکرار باقی مانده نیز\n",
        "  [a,b,c,d,z]\n",
        "  باشد (که این عناصر به ترتیب مرتب شده‌اند). از عناصر پرتکرار باقی مانده، تنها \n",
        "  z\n",
        "  است که از تمام اعضای این مجموعه بزرگ‌تر است. حال یک کاندید معرفی شده، \n",
        "  [b,c,d,z]\n",
        "  است. اما \n",
        "  [a,b,c,d]\n",
        "  به عنوان کاندید مطرح نمی‌شود. فرض کنید پرتکرار باشد. می‌دانیم تمام زیرمجموعه‌های یک مجموعه‌ی پرتکرار نیز پرتکرارند. حال اگر \n",
        "  [a,b,c]\n",
        "  پرتکرار بوده باشد،‌ پس در لیست پرتکرارها موجود است و ترکیب‌های آن را هنگام بررسی \n",
        "  [a,b,c]\n",
        "  می‌سازیم. اما اگر پرتکرار نباشد، پس \n",
        "  [a,b,c,d]\n",
        "  نیز پرتکرار نیست و به درستی آن را نساخته‌ایم. \n",
        " \n",
        "<li dir=\"rtl\">\n",
        "      get_new_frequents\n",
        "</li> \n",
        " این تابع تمام سبدها را به عنوان ورودی می‌گیرد، و \n",
        " itemset\n",
        " های پرتکرار مرحله‌ی جدید را به دست می‌آورد. در این راه از تابع زیر به عنوان تابع\n",
        " map\n",
        " کمک می‌گیرد: \n",
        "\n",
        "<li dir=\"rtl\">\n",
        "  count_freq\n",
        "</li> \n",
        "   این تابع، یک سبد به عنوان ورودی می‌گیرد، و به ازای هر کدام از کاندید‌های آن مرحله، اگر آن کاندید در آن سبد موجود باشد، یک خروجی به صورت \n",
        "   (candidate,1) \n",
        "   می‌سازد و خروجی می‌دهد. در نهایت، با شمردن تمام کاندیدها، می‌توانیم \n",
        "   itemset\n",
        "   های پرتکرار را به دست آوریم. ‌\n",
        " \n",
        " </ul> \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79FZkMt320qH",
        "outputId": "ae30799a-571a-42cd-9989-413f9f817fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------loop start----------------------\n",
            "Itemsets of size  2 , count:  732\n",
            "sample: \n",
            "[((208602, 22010060), 1), ((631633, 900269), 1), ((631357, 631633), 1), ((631357, 900269), 1), ((119, 900171), 1), ((22010117, 100700845), 1), ((900233, 22010117), 1), ((900233, 100700845), 1), ((900149, 900233), 1), ((900149, 22010117), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  3 , count:  1772\n",
            "sample: \n",
            "[((900171, 900233, 22010117), 1), ((900171, 900233, 100700845), 1), ((900171, 22010117, 100700845), 1), ((119, 900149, 900233), 1), ((119, 900149, 22010117), 1), ((119, 900149, 100700845), 1), ((119, 900233, 22010117), 1), ((119, 900233, 100700845), 1), ((119, 22010117, 100700845), 1), ((900124, 900160, 900233), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  4 , count:  3518\n",
            "sample: \n",
            "[((119, 900171, 900233, 22010117), 1), ((119, 900171, 900233, 100700845), 1), ((119, 900171, 22010117, 100700845), 1), ((900149, 900233, 22010117, 100700845), 1), ((119, 900124, 900160, 900233), 1), ((119, 900124, 900160, 22010117), 1), ((119, 900124, 900160, 100700845), 1), ((900124, 900149, 900160, 900233), 1), ((900124, 900149, 900160, 22010117), 1), ((900124, 900149, 900160, 100700845), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  5 , count:  5823\n",
            "sample: \n",
            "[((119, 900149, 900233, 22010117, 100700845), 1), ((900124, 900160, 900233, 22010117, 100700845), 1), ((900149, 900171, 900233, 22010117, 100700845), 1), ((119, 900124, 900149, 900160, 900233), 1), ((119, 900124, 900149, 900160, 22010117), 1), ((119, 900124, 900149, 900160, 100700845), 1), ((119, 900124, 900160, 900171, 900233), 1), ((119, 900124, 900160, 900171, 22010117), 1), ((119, 900124, 900160, 900171, 100700845), 1), ((900124, 900149, 900160, 900171, 900233), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  6 , count:  7960\n",
            "sample: \n",
            "[((119, 900124, 900160, 900233, 22010117, 100700845), 1), ((900124, 900149, 900160, 900233, 22010117, 100700845), 1), ((119, 900149, 900171, 900233, 22010117, 100700845), 1), ((900124, 900160, 900171, 900233, 22010117, 100700845), 1), ((119, 900124, 900149, 900160, 900171, 900233), 1), ((119, 900124, 900149, 900160, 900171, 22010117), 1), ((119, 900124, 900149, 900160, 900171, 100700845), 1), ((114, 900101, 900236, 900237, 22010039, 100700841), 1), ((900142, 900152, 900212, 900222, 900244, 100700866), 1), ((631795, 801710, 900124, 900207, 900243, 22010083), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  7 , count:  8889\n",
            "sample: \n",
            "[((119, 900124, 900149, 900160, 900233, 22010117, 100700845), 1), ((119, 900124, 900160, 900171, 900233, 22010117, 100700845), 1), ((900124, 900149, 900160, 900171, 900233, 22010117, 100700845), 1), ((144, 230103, 631795, 801710, 900207, 900243, 22010083), 1), ((144, 230103, 631795, 900102, 900207, 900243, 22010083), 1), ((230103, 631795, 801710, 900124, 900207, 900243, 22010083), 1), ((230103, 631795, 900102, 900124, 900207, 900243, 22010083), 1), ((144, 801710, 900102, 900124, 900207, 900243, 22010083), 1), ((144, 230103, 801710, 900102, 900218, 900243, 22010083), 1), ((144, 631795, 801710, 900102, 900218, 900243, 22010083), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  8 , count:  8032\n",
            "sample: \n",
            "[((119, 900124, 900149, 900160, 900171, 900233, 22010117, 100700845), 1), ((144, 230103, 631795, 801710, 900124, 900218, 900243, 22010083), 1), ((144, 230103, 631795, 900102, 900124, 900218, 900243, 22010083), 1), ((631795, 801710, 900102, 900124, 900207, 900218, 900243, 22010083), 1), ((230103, 801710, 900102, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 230103, 801710, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 230103, 900102, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 631795, 801710, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 631795, 900102, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 230103, 631795, 801710, 900102, 900124, 900207, 900243), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  9 , count:  5806\n",
            "sample: \n",
            "[((144, 230103, 631795, 801710, 900102, 900124, 900207, 900243, 22010083), 1), ((144, 230103, 631795, 801710, 900102, 900207, 900218, 900243, 22010083), 1), ((230103, 631795, 801710, 900102, 900124, 900207, 900218, 900243, 22010083), 1), ((144, 230103, 801710, 900102, 900124, 900218, 900244, 900277, 22010083), 1), ((144, 631795, 801710, 900102, 900124, 900218, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 900124, 900207, 900218, 900244, 900277, 22010083), 1), ((230103, 631795, 801710, 900102, 900207, 900218, 900244, 900277, 22010083), 1), ((631795, 801710, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((631795, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((230103, 801710, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  10 , count:  3303\n",
            "sample: \n",
            "[((144, 230103, 801710, 900102, 900124, 900207, 900218, 900244, 900277, 22010083), 1), ((144, 631795, 801710, 900102, 900124, 900207, 900218, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 801710, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 900102, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((230103, 631795, 801710, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((230103, 631795, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((144, 801710, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 801710, 900124, 900207, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 900102, 900124, 900207, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 801710, 900124, 900207, 900218, 900243, 900277, 22010083), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  11 , count:  1444\n",
            "sample: \n",
            "[((144, 230103, 631795, 801710, 900102, 900124, 900218, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 801710, 900102, 900124, 900207, 900243, 900244, 900277, 100701266), 1), ((144, 230103, 631795, 801710, 900102, 900207, 900218, 900243, 900244, 900277, 100701266), 1), ((230103, 631795, 801710, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 100701266), 1), ((144, 230103, 631795, 801710, 900102, 900124, 900207, 900218, 900243, 900277, 100701266), 1), ((144, 230103, 631795, 801710, 900124, 900207, 900218, 900243, 900244, 22010083, 100701266), 1), ((144, 230103, 631795, 900102, 900124, 900207, 900218, 900243, 900244, 22010083, 100701266), 1), ((144, 230103, 631795, 801710, 900102, 900124, 900207, 900218, 900277, 22010083, 100701266), 1), ((144, 230103, 801710, 900102, 900124, 900207, 900218, 900243, 900277, 22010083, 100701266), 1), ((144, 631795, 801710, 900102, 900124, 900207, 900218, 900243, 900277, 22010083, 100701266), 1)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  12 , count:  468\n",
            "sample: \n",
            "[((144, 230103, 631795, 801710, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 22010083), 1), ((144, 230103, 631795, 801710, 900124, 900207, 900218, 900243, 900244, 900277, 22010083, 100701266), 1), ((144, 230103, 631795, 900102, 900124, 900207, 900218, 900243, 900244, 900277, 22010083, 100701266), 1), ((230101, 631356, 631831, 900101, 900104, 900152, 900212, 900213, 900217, 900236, 900258, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900152, 900212, 900213, 900217, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900236, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900104, 900142, 900152, 900212, 900213, 900236, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900104, 900142, 900152, 900212, 900217, 900236, 900244, 900258, 100700835), 1), ((230101, 631831, 900101, 900104, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900152, 900213, 900217, 900236, 900244, 900258, 100700835), 1)]\n",
            "-----------------loop start----------------------\n",
            "Itemsets of size  13 , count:  106\n",
            "sample: \n",
            "[((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900236, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900217, 900236, 900244, 900258, 100700835), 1), ((230101, 631356, 631831, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 100700835), 1), ((230101, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((631356, 631831, 900101, 900104, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 900101, 900104, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900104, 900142, 900152, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900104, 900142, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900152, 900212, 900213, 900236, 900244, 900258, 900276, 100700835), 1)]\n",
            "-----------------loop start----------------------\n",
            "Itemsets of size  14 , count:  15\n",
            "sample: \n",
            "[((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 100700835), 1), ((230101, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276), 1), ((631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1), ((230101, 631356, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1)]\n",
            "-----------------loop start----------------------\n",
            "Itemsets of size  15 , count:  1\n",
            "sample: \n",
            "[((230101, 631356, 631831, 900101, 900104, 900142, 900152, 900212, 900213, 900217, 900236, 900244, 900258, 900276, 100700835), 1)]\n",
            "-----------------loop start----------------------\n",
            "Itemsets of size  16 , count:  0\n",
            "sample: \n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def generate_combinations(old_combinations):\n",
        "  \"\"\"\n",
        "  Input old_frequent_itemsets, and create new candidates from it\n",
        "  \"\"\"\n",
        "\n",
        "  def generate_combinations_util(old_combination):\n",
        "    \"\"\"\n",
        "    lambda function that maps an old combination to a number of new candidate combinations\n",
        "    \"\"\"\n",
        "    old_combination = old_combination[0]\n",
        "    old_combination_max_item = old_combination[-1]\n",
        "\n",
        "    # Can do here numpy way\n",
        "    bigger_items = remaining_items[remaining_items>old_combination_max_item]\n",
        "    new_candidates = []\n",
        "    for x in bigger_items:\n",
        "      new_candidates.append( old_combination + (x,) )\n",
        "\n",
        "    return new_candidates\n",
        "\n",
        "  remaining_items = np.array(old_combinations.flatMap(lambda x: x[0]).distinct().sortBy(lambda x: x).collect())\n",
        "  # print('remaining items: ', remaining_items)\n",
        "\n",
        "  new_combinations = old_combinations.flatMap(generate_combinations_util)\n",
        "  return new_combinations\n",
        "\n",
        "def get_new_frequents(candidates):\n",
        "  # frequent_itemsets_rdd = frequent_itemsets_rdd.filter(lambda x: set(x[0]) <= set(x[1])).map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1]>MIN_COUNT)\n",
        "  \n",
        "  _candidates = candidates.collect()\n",
        "  def count_freq(basket):\n",
        "    candidates_present = []\n",
        "    for candidate in _candidates:\n",
        "      if set(candidate) <= set(basket):\n",
        "        candidates_present.append( (candidate,1) )\n",
        "    \n",
        "    return candidates_present\n",
        "\n",
        "  frequent_itemsets_rdd = baskets_rdd.flatMap(count_freq).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1]>MIN_COUNT)\n",
        "  \n",
        "  return frequent_itemsets_rdd\n",
        "\n",
        "k = 1\n",
        "frequent_itemsets_rdd = frequent_items_rdd\n",
        "while frequent_itemsets_rdd.count() != 0:\n",
        "  print('-----------------loop start----------------------')\n",
        "  k += 1\n",
        "  candidates = generate_combinations(frequent_itemsets_rdd)\n",
        "  frequent_itemsets_rdd = get_new_frequents(candidates)\n",
        "  print('Itemsets of size ', k, ', count: ', frequent_itemsets_rdd.count())\n",
        "  print('sample: ')\n",
        "  print(frequent_itemsets_rdd.take(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9ALos9-4a94"
      },
      "outputs": [],
      "source": [
        "# local\n",
        "# sample 0.01 -> thresh = 0.001 -> freq items = 352 -> ~7m, ~9m\n",
        "# sample 1 -> thresh 0.02 -> freq items = 37 -> 5m\n",
        "# sample 1 -> thresh 0.01 -> freq items = 108 -> 39m -> 6 pairs, 0 3ples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A-Priori as a function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        " این کد، الگوریتم را به صورت یک تابع و یکجا درآوره است، تا در مرحله‌ی بعدی از آن در الگوریتم \n",
        " SON\n",
        " استفاده کنیم.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apriori(baskets_rdd, MIN_COUNT, verbose=False):\n",
        "  frequent_items_rdd = baskets_rdd.flatMap(lambda basket: [((item,),1) for item in basket]).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] > MIN_COUNT)\n",
        "\n",
        "  def generate_combinations(old_combinations):\n",
        "    \"\"\"\n",
        "    Input old_frequent_itemsets, and create new candidates from it\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_combinations_util(old_combination):\n",
        "      \"\"\"\n",
        "      lambda function that maps an old combination to a number of new candidate combinations\n",
        "      \"\"\"\n",
        "      old_combination = old_combination[0]\n",
        "      old_combination_max_item = old_combination[-1]\n",
        "\n",
        "      # Can do here numpy way\n",
        "      bigger_items = remaining_items[remaining_items>old_combination_max_item]\n",
        "      new_candidates = []\n",
        "      for x in bigger_items:\n",
        "        new_candidates.append( old_combination + (x,) )\n",
        "\n",
        "      return new_candidates\n",
        "\n",
        "    remaining_items = np.array(old_combinations.flatMap(lambda x: x[0]).distinct().sortBy(lambda x: x).collect())\n",
        "\n",
        "    new_combinations = old_combinations.flatMap(generate_combinations_util)\n",
        "    return new_combinations\n",
        "\n",
        "  def get_new_frequents(candidates):\n",
        "    _candidates = candidates.collect()\n",
        "    def count_freq(basket):\n",
        "      candidates_present = []\n",
        "      for candidate in _candidates:\n",
        "        if set(candidate) <= set(basket):\n",
        "          candidates_present.append( (candidate,1) )\n",
        "      \n",
        "      return candidates_present\n",
        "\n",
        "    frequent_itemsets_rdd = baskets_rdd.flatMap(count_freq).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1]>MIN_COUNT)\n",
        "    \n",
        "    return frequent_itemsets_rdd\n",
        "\n",
        "  if verbose:\n",
        "    print('MIN_COUNT is: ', MIN_COUNT)\n",
        "  k = 1\n",
        "  frequent_itemsets_rdd = frequent_items_rdd\n",
        "  frequent_itemsets_rdds = []\n",
        "  while frequent_itemsets_rdd.count() != 0:\n",
        "    frequent_itemsets_rdds.append(frequent_itemsets_rdd)\n",
        "    if verbose:\n",
        "      print('-----------------loop start----------------------')\n",
        "    k += 1\n",
        "    candidates = generate_combinations(frequent_itemsets_rdd)\n",
        "    frequent_itemsets_rdd = get_new_frequents(candidates)\n",
        "    if verbose:\n",
        "      print('Itemsets of size ', k, ', count: ', frequent_itemsets_rdd.count())\n",
        "      print('sample: ')\n",
        "      print(frequent_itemsets_rdd.take(10))\n",
        "\n",
        "  return frequent_itemsets_rdds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        "کد زیر برای امتحان کردن کارایی تابع \n",
        "apriori\n",
        "است.  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIN_COUNT is:  15\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  2 , count:  811\n",
            "sample: \n",
            "[((101301, 900101), 30), ((900102, 100701100), 36), ((900182, 100701100), 33), ((145, 100700841), 20), ((900222, 900228), 28), ((900222, 100700868), 171), ((100700866, 100700868), 47), ((900234, 900276), 34), ((900235, 100700871), 41), ((209103, 900235), 18)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  3 , count:  184\n",
            "sample: \n",
            "[((900212, 900244, 22009977), 23), ((631765, 900164, 900276), 17), ((631765, 900164, 100700820), 35), ((631765, 900276, 100700820), 21), ((900101, 900259, 100700841), 35), ((900155, 900222, 100700868), 50), ((205802, 900215, 900234), 19), ((142, 900215, 900234), 16), ((205802, 212802, 900233), 16), ((900215, 900234, 900256), 23)]\n",
            "-----------------loop start----------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemsets of size  4 , count:  11\n",
            "sample: \n",
            "[((22010087, 22010088, 22010094, 22010095), 28), ((900101, 900212, 900244, 100700839), 16), ((900193, 900212, 900244, 100700839), 16), ((900102, 900142, 900212, 900244), 18), ((900142, 900202, 900212, 900244), 16), ((900142, 900212, 900244, 900249), 17), ((900142, 900212, 900244, 100700853), 54), ((209103, 900265, 100700804, 100700834), 21), ((900142, 900152, 900212, 900244), 18), ((231, 900236, 900255, 100700841), 20)]\n",
            "-----------------loop start----------------------\n",
            "Itemsets of size  5 , count:  0\n",
            "sample: \n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "SAMPLE = True\n",
        "SAMPLE_SIZE = 0.01\n",
        "THRESHOLD = 0.001\n",
        "\n",
        "\n",
        "baskets_rdd = rdd_group.map(lambda x: x[1])\n",
        "\n",
        "if SAMPLE:\n",
        "  baskets_rdd = baskets_rdd.sample(True, SAMPLE_SIZE)\n",
        "  baskets_rdd = baskets_rdd.coalesce(10)\n",
        "  baskets_rdd.cache()\n",
        "\n",
        "BASKETS_COUNT = baskets_rdd.count()\n",
        "BASKETS_COUNT\n",
        "\n",
        "MIN_COUNT = int(THRESHOLD * BASKETS_COUNT)\n",
        "\n",
        "\n",
        "frequent_itemsets_rdds = apriori(baskets_rdd, MIN_COUNT, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        "کد این قسمت، با استفاده از قسمت قبلی زده شده است. \n",
        "\n",
        "در مورد \n",
        "sample\n",
        "و حداقل ساپورت توضیحات کاملا یکسان است. به هر کدام از ردیف‌های \n",
        "RDD\n",
        "به صورت تصادفی عددی بین ۰ تا تعداد \n",
        "partiotion\n",
        "ها منهی ۱ نسبت می‌دهم، که تعداد \n",
        "partiotion\n",
        "ها نشان می‌دهد که می‌خواهم\n",
        "RDD\n",
        "خود را به چند قسمت تقسیم کنم. سپس، با فیلتر کردن روی عدد نسبت داده شده، \n",
        "RDD\n",
        "من به تعداد\n",
        "partiotion\n",
        "قسمت تقریبا هم اندازه تقسیم می‌شود. حداقل ساپورت هر کدام از این قسمت‌ها را با توجه به اندازه‌شان (که تقریبا برابر هم است) به دست می‌آوریم، دقت کنید که چون اندازه‌ی این قسمت‌ها کوچک شده است،‌حداقل ساپورت نیز به نسبت کم شده است. برای اطمینان نیز حداقل ساپورت را در \n",
        "0.9\n",
        "ضرب می‌کنیم تا چیزی را از دست ندهیم. حال الگوریتم\n",
        "apriori\n",
        "را روی تمامی قسمت‌ها اجرا می‌کنیم. سپس، خروجی این الگوریتم‌ها را با هم \n",
        "union\n",
        "می‌کنیم، تا \n",
        "itemset\n",
        "های پرتکرار برایند الگوریتم‌ها را به دست آوریم. اما این \n",
        "itemset\n",
        "ها ممکن است حاوی\n",
        "<span dir=\"ltr\">\n",
        " false positive \n",
        "</span> \n",
        "باشند. برای همین یک بار دیگر روی کل دیتا حرکت می‌کنیم و این کاندید‌ها را دوباره می‌شماریم و فیلتر می‌کنیم. این قسمت دقیقا شبیه کار مشابه ما در الگوریتم\n",
        "apriori\n",
        "انجام شده است. نتایج به دست آمده از این الگوریتم، دقیقا با الگوریتم \n",
        "apriori\n",
        "یکسان بود (که همین انتظار را نیز داشتیم، زیرا\n",
        "SON\n",
        "نه \n",
        "<span dir=\"ltr\">\n",
        " false positive  \n",
        "</span> \n",
        "دارد و نه \n",
        "<span dir=\"ltr\">\n",
        " false negative \n",
        "</span> \n",
        "). بنابراین \n",
        "نتایج هردو قسمت را در یک بلاک یکسان در انتهای گزارش آورده‌ام. \n",
        "\n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "baskets_rdd = rdd_group.map(lambda x: x[1])\n",
        "\n",
        "SAMPLE = False\n",
        "SAMPLE_SIZE = 0.01\n",
        "# THRESHOLD = 0.01\n",
        "THRESHOLD = 0.02\n",
        "\n",
        "if SAMPLE:\n",
        "  baskets_rdd = baskets_rdd.sample(True, SAMPLE_SIZE)\n",
        "  baskets_rdd = baskets_rdd.coalesce(10)\n",
        "  baskets_rdd.cache()\n",
        "\n",
        "BASKETS_COUNT = baskets_rdd.count()\n",
        "BASKETS_COUNT\n",
        "\n",
        "MIN_COUNT = int(THRESHOLD * baskets_rdd.count())\n",
        "print('MIN_COUNT is: ', MIN_COUNT)\n",
        "partitions_count = 4\n",
        "def random_lambda(x):\n",
        "    return (x, random.randint(0,partitions_count-1))\n",
        "\n",
        "baskets_rdd_with_random = baskets_rdd.map(random_lambda)\n",
        "\n",
        "# print(baskets_rdd.take(10))\n",
        "\n",
        "baskets_rdds = []\n",
        "for i in range(partitions_count):\n",
        "  baskets_rdds.append(baskets_rdd_with_random.filter(lambda x: x[1] == i).map(lambda x: x[0]))\n",
        "\n",
        "\n",
        "MIN_COUNTS = [ int(THRESHOLD * basket_rdd.count()) for basket_rdd in baskets_rdds]\n",
        "print('MIN_COUNTS are: ', MIN_COUNTS)\n",
        "\n",
        "frequent_itemsets_rdds_for_samples = []\n",
        "for i in range(partitions_count):\n",
        "  frequent_itemsets_rdds_for_samples.append(apriori(baskets_rdds[i], int(MIN_COUNTS[i]*0.9)))\n",
        "\n",
        "max_size = max(list(map(len, frequent_itemsets_rdds_for_samples)))\n",
        "\n",
        "\n",
        "for i in range(1, max_size+1):\n",
        "  frequent_itemsets_rdds_for_samples = list(filter(lambda x: len(x) >= i, frequent_itemsets_rdds_for_samples))\n",
        "\n",
        "  frequent_itemsets_rdd = sc.union([x[i-1] for x in frequent_itemsets_rdds_for_samples]).map(lambda x: x[0]).distinct()\n",
        "  _candidates = frequent_itemsets_rdd.collect()\n",
        "  def count_freq(basket):\n",
        "    candidates_present = []\n",
        "    for candidate in _candidates:\n",
        "      if set(candidate) <= set(basket):\n",
        "        candidates_present.append( (candidate,1) )\n",
        "    \n",
        "    return candidates_present\n",
        "\n",
        "  frequent_itemsets_rdd = baskets_rdd.flatMap(count_freq).reduceByKey(lambda x,y: x+y).filter(lambda x: x[1]>MIN_COUNT)\n",
        " \n",
        "  \n",
        "  print('Itemsets of size ', i, ', count: ', frequent_itemsets_rdd.count())\n",
        "  print('sample: ')\n",
        "  print(frequent_itemsets_rdd.take(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3 dir=\"rtl\">\n",
        "روی داده‌های کامل \n",
        "sample\n",
        "داده شده،‌ با استفاده از\n",
        "THRESHOLD\n",
        "برابر با ۲ درصد، یا به عبارتی حداقل ساپورت برابر با \n",
        "۳۰۴۰۸،\n",
        "item\n",
        "های پرتکرار ۲۷ تا بودند، که یک نمونه ۱۰ تایی از آن‌ها \n",
        "\n",
        " <br> \n",
        "\n",
        "<span dir=\"ltr\">\n",
        "((900107,), 38371), ((900155,), 45958), ((900207,), 35631), ((22010119,), 30749), ((900139,), 31477), ((900191,), 31379), ((100700824,), 34662), ((900244,), 61075), ((900164,), 37506), ((900236,), 41652)\n",
        "</span> \n",
        "\n",
        "است. مجموعه‌ی پرتکرار ۲ تایی نیز تنها \n",
        "<span dir=\"ltr\">\n",
        " ((900212, 900244), 42733)\n",
        "</span> \n",
        " وجود داشت (عدد سمت راست هر مجموعه تعداد تکرار آن است). \n",
        "\n",
        "\n",
        " برای این که کدم را برای مجموعه‌های بزرگتر نیز تست کنم، از سمپل داده شده، یک سمپل \n",
        " 0.01\n",
        "  نیز گرفتم، و \n",
        " THRESHOLD\n",
        "را نیز برابر \n",
        " 0.001\n",
        "  قرار دادم. نتیجه‌ی این اجرا نیز به صورت زیر بود:  \n",
        "</h3> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4 >\n",
        "  \n",
        "\n",
        "SAMPLE_SIZE = 0.01\n",
        "\n",
        "THRESHOLD = 0.001\n",
        "\n",
        "MIN_COUNT is:  15\n",
        "\n",
        "\n",
        "\n",
        "Itemsets of size  2 , count:  811\n",
        "\n",
        "sample: \n",
        "\n",
        "[((101301, 900101), 30), ((900102, 100701100), 36), ((900182, 100701100), 33), ((145, 100700841), 20), ((900222, 900228), 28), ((900222, 100700868), 171), ((100700866, 100700868), 47), ((900234, 900276), 34), ((900235, 100700871), 41), ((209103, 900235), 18)]\n",
        "\n",
        "\n",
        "\n",
        "Itemsets of size  3 , count:  184\n",
        "\n",
        "sample: \n",
        "[((900212, 900244, 22009977), 23), ((631765, 900164, 900276), 17), ((631765, 900164, 100700820), 35), ((631765, 900276, 100700820), 21), ((900101, 900259, 100700841), 35), ((900155, 900222, 100700868), 50), ((205802, 900215, 900234), 19), ((142, 900215, 900234), 16), ((205802, 212802, 900233), 16), ((900215, 900234, 900256), 23)]\n",
        "\n",
        "\n",
        "\n",
        "Itemsets of size  4 , count:  11\n",
        "\n",
        "sample: \n",
        "[((22010087, 22010088, 22010094, 22010095), 28), ((900101, 900212, 900244, 100700839), 16), ((900193, 900212, 900244, 100700839), 16), ((900102, 900142, 900212, 900244), 18), ((900142, 900202, 900212, 900244), 16), ((900142, 900212, 900244, 900249), 17), ((900142, 900212, 900244, 100700853), 54), ((209103, 900265, 100700804, 100700834), 21), ((900142, 900152, 900212, 900244), 18), ((231, 900236, 900255, 100700841), 20)]\n",
        "\n",
        "</h4> "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MDA_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
